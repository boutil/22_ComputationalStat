\documentclass[a4paper,10pt,fleqn]{article}

\usepackage{a4wide,amsmath,amsthm,amssymb,bbm,fancyhdr}
\usepackage{ifthen,color,enumerate,comment,dsfont,pdfsync,framed,todonotes,enumitem}

\newcommand{\titre}[1]{\textbf{\textsc{#1}}}

\RequirePackage[T1]{fontenc}

\usepackage[latin1]{inputenc}
\usepackage{graphicx}
\usepackage{dsfont}
\usepackage{enumitem}
\newcommand{\eqsp}{\,}
\newcommand{\R}{\ensuremath{\mathbb{R}}}
\newcommand{\calF}{\mathcal{F}}
\newcommand{\rmd}{\mathrm{d}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\rset}{\ensuremath{\mathbb{R}}}
\renewcommand{\P}{\ensuremath{\operatorname{P}}}
\newcommand{\bP}{\mathbb{P}}
\newcommand{\E}{\ensuremath{\mathbb{E}}}
\newcommand{\rme}{\ensuremath{\mathrm{e}}}
\newcommand{\calH}{\ensuremath{\mathcal{H}}}
\newcommand{\xset}{\ensuremath{\mathsf{X}}}
\newcommand{\V}{\ensuremath{\mathbb{V}}}
\newcommand{\Sb}{\ensuremath{\mathbb{S}}}
\newcommand{\gaus}{\ensuremath{\mathcal{N}}}
\newcommand{\HH}{\ensuremath{\mathcal{H}}}
\newcommand{\F}{\ensuremath{\mathcal{F}}}
\newcommand{\W}{\ensuremath{\mathcal{W}}}
\newcommand{\X}{\ensuremath{\mathcal{X}}}
\newcommand{\1}{\ensuremath{\mathbbm{1}}}
\newcommand{\dlim}{\ensuremath{\stackrel{\mathcal{L}}{\longrightarrow}}}
\newcommand{\plim}{\ensuremath{\stackrel{\mathrm{P}}{\longrightarrow}}}
\newcommand{\PP}{\ensuremath{\mathbb{P}}}
\newcommand{\p}{\ensuremath{\mathbb{P}}}
\newcommand{\eps}{\varepsilon}
\newcommand{\bE}{\mathbb{E}}
\newcommand{\pa}[1]{\left(#1\right)}
\newcommand{\hatk}{\widehat K}
\newcommand{\f}{\varphi}
\newcommand{\Id}{\textsf{Id}}
\newcommand{\bfU}{\mathbf{U}}
\newcommand{\bfX}{\mathbf{X}}
\newcommand{\bfs}{\mathbf{\Sigma}}
\newcommand{\bfA}{\mathbf{A}}
\newcommand{\bfV}{\mathbf{V}}
\newcommand{\bfB}{\mathbf{B}}
\newcommand{\bfI}{\mathbf{I}}
\newcommand{\bfD}{\mathbf{D}}
\newcommand{\bfK}{\mathbf{K}}
\newcommand{\argmin}{\mathop{\textrm{argmin}}}
\newcommand{\argmax}{\mathop{\textrm{argmax}}}
\newcommand{\crit}{\mathop{\textrm{crit}}}
\newcommand{\C}{\mathcal{C}}
\newcommand{\pc}{\pi_{\mathcal{C}}}


% Style


\newtheorem{theorem}{Theorem}


\begin{document}

%\noindent Statistiques computationnelles \hfill Sorbonne Universit\'e \\


\noindent\hrulefill

\begin{center}
\textsc{Expectation Maximization pour les m\'elanges de lois de Poisson}
\end{center}
\hrulefill

\medskip

\begin{enumerate}
\item Consid\'erons $\{V_k\}_{1\leq k\leq K}$ ind\'ependantes et telles que $V_k\sim \mathcal{P}(\lambda_k)$ pour $1\leq k\leq K$ et $Z$ une variable al\'eatoire de loi multinomiale de param\`etres $\{\pi_1, \ldots,\pi_K\}$ ind\'ependante des $\{V_k\}_{1\leq k\leq K}$. Il suffit alors de poser $X = V_Z = \sum_{k=1}^K \mathds{1}_{Z=k}V_k$.
\item Par d\'efinition, pour tout $j\geq 0$,
\begin{align*}
\mathbb{P}_\theta\left(X=j\right) &= \sum_{k=1}^K \mathbb{P}_\theta\left(X=j \middle |Z = k\right)\mathbb{P}_\theta\left(Z = k\right)\,,\\
&=  \sum_{k=1}^K \mathrm{e}^{-\lambda_k}\frac{\lambda_k^j}{j!}\pi_k\,.
\end{align*}
\item \'Ecrivons la logvraisemblance:
\begin{align*}
\log \mathbb{P}_\theta\left(X_{1:n}=x_{1:n}\right) &= \sum_{i=1}^n\log \mathbb{P}_\theta\left(X_{i}=x_{i}\right)\,,\\
&=\sum_{i=1}^n\log \left(\sum_{k=1}^K \mathrm{e}^{-\lambda_k}\frac{\lambda_k^{x_i}}{x_i!}\pi_k\right)\,,\\
&=\sum_{i=1}^n\left\{-\log  x_i! + \log \left(\sum_{k=1}^K \mathrm{e}^{-\lambda_k}\lambda_k^{x_i} \pi_k\right)\right\}\,.
\end{align*}
On remarque ensuite ais\'ement que l'\'equation $\nabla_\theta \log \mathbb{P}_\theta\left(X_{1:n}=x_{1:n}\right)  = 0$ n'admet pas de solution explicite.
\item
\begin{enumerate}
\item Pour tout $\theta$ et tout $k\in\{1,\ldots,K\}$, $j\geq 0$,
$$
\mathbb{P}_\theta\left(Z=k\middle|X=j\right) = \frac{\mathbb{P}_\theta\left(Z=k;X=j\right)}{\mathbb{P}_\theta\left(X=j\right)} = \frac{\pi_k \mathrm{e}^{-\lambda_k}\lambda_k^{j}/j!}{\sum_{\ell}^K\pi_\ell \mathrm{e}^{-\lambda_\ell}\lambda_\ell^{j}/j!}\,.
$$
\item La logvraisemblance compl\`ete des donn\'ees est :
$$
\mathcal{L}(x,z;\theta) = \sum_{k=1}^K \mathds{1}_{z=k}\left\{\log \pi_k + \log \left(\mathrm{e}^{-\lambda_k}\frac{\lambda_k^{x}}{x!}\right)\right\}\,.
$$
Puisque les donn\'ees sont ind\'ependantes, on obtient,
\begin{align*}
\mathcal{L}(x,z;\theta) &= \sum_{i=1}^n\sum_{k=1}^K \mathds{1}_{z_i=k}\left\{\log \pi_k + \log \left(\mathrm{e}^{-\lambda_k}\frac{\lambda_k^{x_i}}{x_i!}\right)\right\}\,,\\
&=\sum_{i=1}^n\sum_{k=1}^K \mathds{1}_{z_i=k}\left\{\log \pi_k - \lambda_k+ x_i\log \left(\lambda_k\right) - \log x_i!\right\}\,.
\end{align*}
\item Pour tout $\theta, \theta'$,
\begin{align*}
Q(\theta;\theta') &= \mathbb{E}_{\theta'}\left[\mathcal{L}(X_{1:n},Z_{1:n};\theta)\middle|X_{1:n}\right] \,,\\
&= \sum_{i=1}^n\sum_{k=1}^K \mathbb{E}_{\theta'}\left[\mathds{1}_{Z_i=k}\middle|X_{1:n}\right]\left\{\log \pi_k - \lambda_k+ X_i\log \left(\lambda_k\right) - \log X_i!\right\}\,,\\
&= \sum_{i=1}^n\sum_{k=1}^K \mathbb{P}_{\theta'}\left(Z_i=k\middle|X_{i}\right)\left\{\log \pi_k - \lambda_k+ X_i\log \left(\lambda_k\right) - \log X_i!\right\}
\end{align*}
\`A l'it\'eration $p\geq 0$ de l'algorithme, si nous disposons d'un estimateur courant $\theta^{(p)}$, nous calculons
$$
\omega_{i,k}^{(p)}(X_i) = \mathbb{P}_{\theta^{(p)}}\left(Z_i=k\middle|X_{i}\right) = \frac{\pi^{(p)}_k \mathrm{e}^{-\lambda^{(p)}_k}(\lambda^{(p)}_k)^{j}/j!}{\sum_{\ell}^K\pi^{(p)}_\ell \mathrm{e}^{-\lambda^{(p)}_\ell}(\lambda^{(p)}_\ell)^{j}/j!}
$$
et 
$$
Q(\theta;\theta^{(p)}) = \sum_{i=1}^n\sum_{k=1}^K \omega_{i,k}^{(p)}(X_i) \left\{\log \pi_k - \lambda_k+ X_i\log \left(\lambda_k\right) - \log X_i!\right\}\,.
$$
\item Il est ais\'e de montrer que la fonction $\theta \mapsto Q(\theta;\theta^{(p)}) $ admet un maximum unique, obtenu en r\'esolvant l'\'equation $\nabla_\theta Q(\theta;\theta^{(p)}) = 0$. Pour tout $1\leq k \leq K$,
$$
\partial_{\lambda_k}Q(\theta;\theta^{(p)}) = \sum_{i=1}^n\omega_{i,k}^{(p)}(X_i) \left\{ - 1+ \frac{X_i}{\lambda_k}\right\}\,.
$$
On en d\'eduit que
$$
\lambda^{(p+1)}_k = \frac{\sum_{i=1}^n\omega_{i,k}^{(p)}(X_i)X_i}{\sum_{i=1}^n\omega_{i,k}^{(p)}(X_i)}\,.
$$
Par ailleurs, pour tout $1\leq k \leq K-1$, en utilisant que $\pi_K = 1 - \sum_{j=1}^{K-1}\pi_j$,
$$
\partial_{\pi_k}Q(\theta;\theta^{(p)}) = \sum_{i=1}^n \left\{\frac{\omega_{i,k}^{(p)}(X_i)}{\pi_k} - \frac{\omega_{i,k}^{(p)}(X_i)}{\pi_K}\right\}
$$
et on en d\'eduit que $k\mapsto\sum_{i=1}^n\omega_{i,k}^{(p)}(X_i)/\pi_k$ est constante. En utilisant par ailleurs que  $\sum_{k=1}^{K}\pi_k = 1$ et $\sum_{k=1}^{K}\omega_{i,k}^{(p)}(X_i)= 1$, on a
$$
\pi_k^{(p+1)} = \frac{1}{n}\sum_{i=1}^n\omega_{i,k}^{(p)}(X_i)\,.
$$
\item Pour mettre en place l'algorithme EM, il suffit d'initialiser l'algorithme avec une valeur $\theta^{(0)}$ puis \`a chaque it\'eration $p\geq 0$ d'effectuer l'\'etape E (i.e. calculer $\omega_{i,k}^{(p)}$ pour $1\leq i \leq n$ et $1\leq k \leq K$) et de calculer $\theta^{(p+1)}$ en appliquant les mises \`a jour de la question pr\'ec\'edente.
\item La seule garantie que nous avons est que la vraisemblance des observations augmente \`a chaque it\'eration. En pratique, l'algorithme converge vers un maximum local, et il faut donc analyser les diff\'erents points de convergence obtenus si on initialise l'algorithme de diff\'erentes fa\c cons. 
\end{enumerate}
\end{enumerate}
\end{document}